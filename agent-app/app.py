"""
Aplicaci√≥n Streamlit - Asistente Inteligente Manuelita

3 Ventanas: FAQs, Admin, Chat Interactivo
"""

import streamlit as st
import time
from datetime import datetime
from typing import Dict, Any, List
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from agent import ManuelitaAgent
from config import config, SAMPLE_FAQS
from memory import SessionManager
from parser import create_faq_json

# ============================================================================
# CONFIGURACI√ìN STREAMLIT
# ============================================================================

st.set_page_config(
    page_title="Manuelita Assistant",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# INICIALIZACI√ìN DE ESTADO
# ============================================================================

if 'current_provider' not in st.session_state:
    st.session_state.current_provider = config.llm.provider

if 'agent' not in st.session_state:
    st.session_state.agent = ManuelitaAgent(provider=st.session_state.current_provider)
else:
    # Si el proveedor cambi√≥ en config, reinicializa el agente
    if config.llm.provider != st.session_state.current_provider:
        st.session_state.current_provider = config.llm.provider
        st.session_state.agent = ManuelitaAgent(provider=config.llm.provider)

if 'session_manager' not in st.session_state:
    st.session_state.session_manager = SessionManager()
    st.session_state.session_manager.create_conversation("Conversaci√≥n 1")

if 'current_conversation' not in st.session_state:
    st.session_state.current_conversation = "Conversaci√≥n 1"

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

if 'json_generated' not in st.session_state:
    st.session_state.json_generated = False

# ============================================================================
# FUNCIONES AUXILIARES
# ============================================================================

def stream_response(text: str, speed_ms: int = 50) -> None:
    """Simula streaming de respuesta car√°cter por car√°cter con icono generador.
    
    Args:
        text: Texto a mostrar
        speed_ms: Velocidad en ms por car√°cter (10=r√°pido, 200=lento)
    """
    delay_sec = speed_ms / 1000.0
    placeholder = st.empty()
    streamed_text = ""
    
    # Streaming con icono como "cursor" generador
    for char in text:
        streamed_text += char
        # Mostrar texto + icono al final (como cursor)
        placeholder.markdown(f"{streamed_text} {config.streaming.icon}")
        time.sleep(delay_sec)
    
    # Resultado final: solo el texto, sin icono
    placeholder.markdown(streamed_text)

def process_user_input(question: str, memory_context: str = "") -> Dict[str, Any]:
    """Procesa entrada del usuario.
    
    Args:
        question: Pregunta del usuario
        memory_context: Contexto de memoria de la conversaci√≥n actual
    """
    try:
        # Inyectar memoria de conversaci√≥n en el agente temporalmente
        original_memory_context = st.session_state.agent.get_memory_context()
        
        # Si hay contexto de conversaci√≥n, usarlo para generar respuesta
        if memory_context:
            # Modificar temporalmente get_memory_context para devolver el contexto actual
            st.session_state.agent.get_memory_context = lambda: memory_context
        
        result = st.session_state.agent.process(
            question=question,
            use_memory=False,  # No usar memoria del agente, pasamos contexto manualmente
            temperature=config.llm.temperature,
            top_k=config.llm.top_k,
            max_tokens=config.llm.max_tokens
        )
        
        # Restaurar m√©todo original
        if memory_context:
            st.session_state.agent.get_memory_context = lambda: original_memory_context
        
        return result
    except Exception as e:
        logger.error(f"Error procesando input: {e}")
        return {
            'question': question,
            'answer': f"Error: {str(e)}",
            'tool_used': 'error',
            'success': False
        }

def generate_faq_json() -> bool:
    """Genera JSON de FAQ desde markdown."""
    try:
        success = create_faq_json(
            markdown_dir=config.data_dir,
            output_path=config.structured_data_file
        )
        return success
    except Exception as e:
        logger.error(f"Error generando FAQ JSON: {e}")
        return False

# ============================================================================
# VENTANA 1: FAQs
# ============================================================================

def page_faqs():
    """P√°gina de FAQs autogeneradas."""
    st.header("‚ùì Preguntas Frecuentes")
    
    col1, col2 = st.columns([4, 1])
    
    with col1:
        st.subheader("Ejemplo de Consultas por Tipo")
    
    with col2:
        if st.button("üîÑ Generar FAQs"):
            with st.spinner("Generando FAQs desde documentos..."):
                if generate_faq_json():
                    st.success("‚úÖ FAQs generadas exitosamente")
                    st.session_state.json_generated = True
                else:
                    st.error("‚ùå Error generando FAQs")
    
    # Mostrar FAQs
    for i, faq in enumerate(SAMPLE_FAQS, 1):
        with st.container():
            col1, col2, col3 = st.columns([0.5, 3, 1.5])
            
            with col1:
                if faq['type'] == 'rag':
                    st.write("üìö")
                elif faq['type'] == 'memory':
                    st.write("üß†")
                elif faq['type'] == 'structured':
                    st.write("üìä")
                else:
                    st.write("üîÄ")
            
            with col2:
                st.write(f"**{faq['question']}**")
                st.caption(faq['description'])
            
            with col3:
                if st.button("Preguntar", key=f"faq_{i}"):
                    st.session_state.user_input = faq['question']
                    st.rerun()
    
    st.divider()
    st.info("üí° Selecciona una pregunta o escribe la tuya en la ventana de Chat")

# ============================================================================
# VENTANA 2: ADMINISTRACI√ìN
# ============================================================================

def page_admin():
    """P√°gina de administraci√≥n."""
    st.header("‚öôÔ∏è Panel de Administraci√≥n")
    
    tab1, tab2 = st.tabs(
        ["‚öôÔ∏è Configuraci√≥n", "üîß Herramientas del Core"]
    )
    
    # TAB 1: CONFIGURACI√ìN
    with tab1:
        st.subheader("Configuraci√≥n de Par√°metros")
        
        # MOSTRAR VALORES ACTUALES EN LA PARTE SUPERIOR
        st.info(
            f"üìä **Configuraci√≥n Activa:**\n\n"
            f"‚Ä¢ Proveedor: **{config.llm.provider}** | Modelo: **{config.llm.model}**\n"
            f"‚Ä¢ Temperatura: **{config.llm.temperature}** | Top-K: **{config.llm.top_k}** | Max Tokens: **{config.llm.max_tokens}**\n"
            f"‚Ä¢ Memoria: **{config.memory.max_tokens} tokens** | Turnos: **{config.memory.max_turns}**\n"
            f"‚Ä¢ Streaming: **{'Activo' if config.streaming.enabled else 'Inactivo'}** ({config.streaming.speed_ms}ms)"
        )
        
        st.divider()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**LLM Settings**")
            
            # Model Selection
            st.markdown("**Proveedor y Modelo**")
            
            # Obtener el √≠ndice correcto del proveedor actual
            providers = ["OpenAI", "Google Gemini", "Ollama"]
            current_provider_index = 0
            if "gemini" in config.llm.provider.lower() or "google" in config.llm.provider.lower():
                current_provider_index = 1
            elif "ollama" in config.llm.provider.lower():
                current_provider_index = 2
            
            provider = st.selectbox(
                "Proveedor de LLM",
                options=providers,
                index=current_provider_index
            )
            
            if provider == "OpenAI":
                models = config.get_openai_models()
            elif provider == "Google Gemini":
                models = config.get_google_models()
            else:  # Ollama
                models = config.get_ollama_models()
            
            selected_model = st.selectbox(
                "Modelo",
                options=models,
                index=models.index(config.llm.model) if config.llm.model in models else 0
            )
            
            if selected_model != config.llm.model:
                config.llm.model = selected_model
                config.llm.provider = provider  # Guardar proveedor seleccionado
                # Reinicializar agente con nuevo modelo Y proveedor
                st.session_state.agent = ManuelitaAgent(provider=provider)
                st.success(f"‚úÖ Modelo cambiado a: {selected_model} ({provider})")
                st.rerun()
            
            st.divider()
            
            # Temperature, Top K, Max Tokens
            st.markdown("**Par√°metros de Generaci√≥n**")
            new_temp = st.slider(
                "Temperatura (0.0-1.0)",
                min_value=0.0,
                max_value=1.0,
                value=float(config.llm.temperature),
                step=0.05,
                help="Controla la aleatoriedad: 0=determinista, 1=creativo"
            )
            if new_temp != config.llm.temperature:
                config.llm.temperature = new_temp
                # Actualizar LLM si es posible
                if st.session_state.agent.llm:
                    st.session_state.agent.llm.temperature = new_temp
            
            new_top_k = st.number_input(
                "Top K (documentos RAG)",
                min_value=1,
                max_value=10,
                value=int(config.llm.top_k),
                help="N√∫mero de chunks a recuperar del RAG"
            )
            if new_top_k != config.llm.top_k:
                config.llm.top_k = new_top_k
            
            new_max_tokens = st.number_input(
                "Max Tokens (respuesta)",
                min_value=100,
                max_value=2000,
                value=int(config.llm.max_tokens),
                step=100,
                help="L√≠mite de tokens para la respuesta generada"
            )
            if new_max_tokens != config.llm.max_tokens:
                config.llm.max_tokens = new_max_tokens
        
        with col2:
            st.write("**Streaming Settings**")
            config.streaming.enabled = st.checkbox(
                "Streaming Activo",
                value=config.streaming.enabled
            )
            config.streaming.speed_ms = st.slider(
                "Velocidad de Streaming (Delay ms por car√°cter)",
                min_value=10,
                max_value=200,
                value=config.streaming.speed_ms,
                step=10,
                help="Menor = m√°s r√°pido, Mayor = m√°s lento. Ej: 10ms r√°pido, 100ms normal, 200ms lento"
            )
            config.streaming.icon = st.selectbox(
                "Icono de Streaming",
                options=config.get_icon_options(),
                index=config.get_icon_options().index(config.streaming.icon)
            )
        
        st.divider()
        st.write("**Memoria Settings**")
        col1, col2 = st.columns(2)
        with col1:
            new_mem_tokens = st.number_input(
                "Max Tokens Memoria",
                min_value=5000,
                max_value=50000,
                value=int(config.memory.max_tokens),
                step=5000,
                help="L√≠mite de tokens para contexto conversacional"
            )
            if new_mem_tokens != config.memory.max_tokens:
                config.memory.max_tokens = new_mem_tokens
                # Actualizar memoria del agente
                st.session_state.agent.memory.max_tokens = new_mem_tokens
        with col2:
            new_mem_turns = st.number_input(
                "Max Turnos Memoria",
                min_value=10,
                max_value=100,
                value=int(config.memory.max_turns),
                step=5,
                help="N√∫mero m√°ximo de intercambios a recordar"
            )
            if new_mem_turns != config.memory.max_turns:
                config.memory.max_turns = new_mem_turns
                st.session_state.agent.memory.max_turns = new_mem_turns
        
        st.success("‚úÖ Configuraci√≥n actualizada y aplicada al agente")
    
    # TAB 2: HERRAMIENTAS DEL CORE
    with tab2:
        st.subheader("üîß Arquitectura y Herramientas del Core")
        
        # SECCI√ìN 1: MODELO Y CONFIGURACI√ìN ACTUAL
        st.markdown("### ü§ñ Modelo LLM Activo")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Proveedor", config.llm.provider)
        with col2:
            st.metric("Modelo", config.llm.model)
        with col3:
            st.metric("Temperatura", f"{config.llm.temperature}")
        
        st.divider()
        
        # SECCI√ìN 2: FLUJO DE PROCESAMIENTO
        st.markdown("### üîÄ Pipeline de Procesamiento")
        st.markdown("""
        ```
        Usuario ‚Üí Router ‚Üí [RAG / Structured Tool] ‚Üí Contexto ‚Üí LLM ‚Üí Respuesta
        ```
        """)
        
        stats = st.session_state.agent.get_agent_stats()
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Router", "‚úÖ Activo")
        with col2:
            st.metric("RAG", "‚úÖ" if stats['rag_available'] else "‚ùå")
        with col3:
            st.metric("Structured", "‚úÖ" if stats['structured_tool_available'] else "‚ùå")
        with col4:
            st.metric("LLM", "‚úÖ" if stats['llm_available'] else "‚ùå")
        
        st.divider()
        
        # SECCI√ìN 3: SISTEMA RAG DETALLADO
        st.markdown("### üìö Sistema RAG (Retrieval-Augmented Generation)")
        
        if st.session_state.agent.rag:
            rag = st.session_state.agent.rag
            rag_stats = rag.get_stats()
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Configuraci√≥n de Embeddings**")
                st.code(f"Modelo: {rag.embedding_model_name}", language="text")
                st.code(f"Base Vectorial: ChromaDB", language="text")
                st.code(f"Directorio: {rag.vectordb_dir}", language="text")
                
                st.markdown("**Estad√≠sticas de Documentos**")
                st.metric("Total Documentos", rag_stats['total_documents'])
                st.metric("Total Chunks", rag_stats['total_chunks'])
            
            with col2:
                st.markdown("**Estrategia de B√∫squeda**")
                st.info(
                    "üîç **B√∫squeda H√≠brida**\n\n"
                    "‚Ä¢ Vectorial (75%): Sem√°ntica con embeddings\n"
                    "‚Ä¢ BM25 (25%): Keyword matching\n"
                    "‚Ä¢ Re-ranking: CrossEncoder (BAAI/bge-reranker-base)\n"
                    "‚Ä¢ Top-K final: 4 chunks"
                )
            
            st.divider()
            
            # VISUALIZAR CHUNKS REALES
            st.markdown("**üîç Explorar Chunks en la Base Vectorial**")
            
            if rag.splits:
                num_samples = st.slider(
                    "N√∫mero de chunks a visualizar",
                    min_value=1,
                    max_value=min(10, len(rag.splits)),
                    value=min(5, len(rag.splits))
                )
                
                st.caption(f"Mostrando {num_samples} de {len(rag.splits)} chunks disponibles")
                
                for i, chunk in enumerate(rag.splits[:num_samples], 1):
                    with st.expander(f"üìÑ Chunk {i} - {chunk.metadata.get('source', 'Unknown')}"):
                        st.markdown("**Metadata:**")
                        st.json(chunk.metadata)
                        st.markdown("**Contenido:**")
                        st.text_area(
                            "Texto del chunk",
                            value=chunk.page_content,
                            height=200,
                            key=f"chunk_{i}",
                            disabled=True
                        )
            else:
                st.warning("No hay chunks disponibles en la base vectorial")
            
            # TEST DE B√öSQUEDA
            st.divider()
            st.markdown("**üß™ Test de B√∫squeda RAG**")
            test_query = st.text_input(
                "Ingresa una consulta para probar el retriever",
                placeholder="Ej: ¬øQu√© es la pol√≠tica de devoluciones?"
            )
            
            if test_query:
                with st.spinner("Buscando chunks relevantes..."):
                    try:
                        search_result = rag.search(test_query, top_k=config.llm.top_k)
                        
                        # El m√©todo search retorna un dict con 'documents' como lista
                        documents = search_result.get('documents', [])
                        context = search_result.get('context', '')
                        
                        if documents:
                            st.success(f"‚úÖ {len(documents)} chunks recuperados")
                            
                            # Mostrar contexto consolidado
                            with st.expander("üìñ Contexto Consolidado (lo que ve el LLM)"):
                                st.text_area(
                                    "Contexto completo",
                                    value=context,
                                    height=200,
                                    disabled=True
                                )
                            
                            st.divider()
                            st.markdown("**Documentos Individuales:**")
                            
                            for idx, doc in enumerate(documents, 1):
                                with st.expander(
                                    f"üéØ Resultado {idx} - {doc.get('relevance', 'N/A')} | Rank: {doc.get('rank', idx)}"
                                ):
                                    st.markdown(f"**Fuente:** `{doc.get('source', 'Unknown')}`")
                                    st.markdown(f"**Relevancia:** {doc.get('relevance', 'N/A')}")
                                    st.markdown(f"**Contenido (preview):**")
                                    st.write(doc.get('content', 'Sin contenido'))
                        else:
                            st.warning("‚ö†Ô∏è No se encontraron resultados para esta consulta")
                    except Exception as e:
                        st.error(f"‚ùå Error en b√∫squeda: {str(e)}")
                        logger.error(f"Error en test de b√∫squeda RAG: {e}")
        else:
            st.warning("‚ùå Sistema RAG no disponible")
        
        st.divider()
        
        # SECCI√ìN 4: HERRAMIENTA ESTRUCTURADA
        st.markdown("### üìä Herramienta de Datos Estructurados")
        
        if st.session_state.agent.structured_tool:
            structured = st.session_state.agent.structured_tool
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Consultas Disponibles**")
                queries = structured.get_available_queries()
                for query in queries:
                    st.code(f"‚Ä¢ {query}", language="text")
            
            with col2:
                st.markdown("**Caracter√≠sticas**")
                st.info(
                    "‚Ä¢ Respuestas estructuradas en JSON\n"
                    "‚Ä¢ B√∫squeda exacta por palabras clave\n"
                    "‚Ä¢ Ideal para FAQs y datos tabulares\n"
                    "‚Ä¢ Sin necesidad de embeddings"
                )
        else:
            st.warning("‚ùå Herramienta estructurada no disponible")
        
        st.divider()
        
        # SECCI√ìN 5: CONEXI√ìN CON VENTANA CHAT
        st.markdown("### üí¨ Integraci√≥n con Ventana de Chat")
        st.info(
            "**Flujo de Interacci√≥n:**\n\n"
            "1. Usuario escribe pregunta en Chat\n"
            "2. Router analiza la pregunta y selecciona herramienta (RAG/Structured)\n"
            "3. Herramienta recupera contexto relevante\n"
            "4. LLM genera respuesta usando el contexto\n"
            "5. Respuesta se muestra en Chat con streaming\n"
            "6. Interacci√≥n se guarda en memoria de conversaci√≥n\n\n"
            "**Memoria:** FIFO con l√≠mite de 20K tokens por conversaci√≥n"
        )
        
        # VISUALIZAR ESTADO ACTUAL
        st.markdown("**Estado de Conversaciones Activas**")
        session_stats = st.session_state.session_manager.get_session_stats()
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Conversaciones Activas", session_stats['total_conversations'])
        with col2:
            st.metric("Turnos Totales", session_stats['total_turns_in_session'])

# ============================================================================
# VENTANA 3: CHAT INTERACTIVO
# ============================================================================

def page_chat():
    """P√°gina de chat interactivo."""
    
    # VALIDAR API KEY
    import os
    has_openai_key = bool(os.getenv("OPENAI_API_KEY"))
    has_google_key = bool(os.getenv("GOOGLE_API_KEY"))
    
    if not has_openai_key and not has_google_key:
        st.error(
            "‚ùå **No hay API Keys configuradas**\n\n"
            "Para usar el chat, necesitas configurar una API key. "
            "Ver: CHAT_TROUBLESHOOTING.md para instrucciones."
        )
        st.info(
            "**Opciones:**\n"
            "1. Crea archivo `.env` con OPENAI_API_KEY o GOOGLE_API_KEY\n"
            "2. O configura variables de entorno: `$env:OPENAI_API_KEY = 'tu-key'`\n"
            "3. Reinicia Streamlit despu√©s de configurar"
        )
        return
    
    if not st.session_state.agent.llm:
        st.warning(
            "‚ö†Ô∏è **LLM no inicializado**\n\n"
            "El modelo LLM no se pudo cargar. Verifica que:\n"
            "- Tengas `langchain-openai` o `langchain-google-genai` instalado\n"
            "- Tu API key sea v√°lida\n"
            "- Reinicia la aplicaci√≥n si acabas de configurar la API key"
        )
    
    # SIDEBAR
    with st.sidebar:
        st.header("üí¨ Conversaciones")
        
        # Crear nueva conversaci√≥n
        if st.button("‚ûï Nueva Conversaci√≥n"):
            conv_name = f"Conversaci√≥n {len(st.session_state.session_manager.conversations) + 1}"
            st.session_state.session_manager.create_conversation(conv_name)
            st.session_state.current_conversation = conv_name
            st.session_state.chat_history = []
            st.rerun()
        
        st.divider()
        
        # Listar conversaciones
        conversations = st.session_state.session_manager.list_conversations()
        for conv_id in conversations:
            col1, col2 = st.columns([3, 1])
            with col1:
                if st.button(conv_id, key=f"conv_{conv_id}"):
                    st.session_state.current_conversation = conv_id
                    st.rerun()
            with col2:
                if st.button("üóëÔ∏è", key=f"del_{conv_id}"):
                    st.session_state.session_manager.delete_conversation(conv_id)
                    st.rerun()
        
        st.divider()
        
        # Stats
        session_stats = st.session_state.session_manager.get_session_stats()
        st.metric("Conversaciones", session_stats['total_conversations'])
        st.metric("Turnos Totales", session_stats['total_turns_in_session'])
    
    # CHAT PRINCIPAL
    st.header(f"üí¨ {st.session_state.current_conversation}")
    
    # Obtener memoria de la conversaci√≥n actual
    current_conv_memory = st.session_state.session_manager.get_conversation(
        st.session_state.current_conversation
    )
    
    if current_conv_memory is None:
        st.error("‚ùå Conversaci√≥n no encontrada. Por favor, selecciona una conversaci√≥n v√°lida.")
        return
    
    # Mostrar historial de la conversaci√≥n actual
    turns = current_conv_memory.get_all_turns()
    
    for turn in turns:
        with st.chat_message("user"):
            st.write(turn.user_question)
        
        with st.chat_message("assistant"):
            st.write(turn.bot_response)
            if turn.sources:
                st.caption(f"üìö Fuentes: {', '.join(turn.sources)}")
    
    # Input del usuario
    user_input = st.chat_input("Escribe tu pregunta...")
    
    if user_input:
        # Mostrar pregunta
        with st.chat_message("user"):
            st.write(user_input)
        
        # Procesar
        with st.chat_message("assistant"):
            with st.spinner("Procesando..."):
                # Pasar contexto de memoria de la conversaci√≥n actual
                memory_ctx = current_conv_memory.get_conversation_context()
                result = process_user_input(user_input, memory_context=memory_ctx)
            
            # Guardar en la memoria de la conversaci√≥n actual
            if result['success']:
                current_conv_memory.add_turn(
                    user_question=user_input,
                    bot_response=result['answer'],
                    rag_context=result.get('context_used', ''),
                    sources=result.get('sources', []),
                    tool_used=result.get('tool_used', 'unknown')
                )
            
            if config.streaming.enabled and result['success']:
                stream_response(result['answer'], speed_ms=config.streaming.speed_ms)
            else:
                st.write(result['answer'])
            
            if result.get('sources'):
                st.caption(f"üìö {result['tool_used'].upper()}: {', '.join(result['sources'])}")

# ============================================================================
# NAVEGACI√ìN PRINCIPAL
# ============================================================================

st.sidebar.title("üè† Navegaci√≥n")
page = st.sidebar.radio(
    "Selecciona una secci√≥n:",
    ["üí¨ Chat", "‚öôÔ∏è Admin", "‚ùì FAQs"]
)

st.sidebar.divider()
st.sidebar.info(
    "üìå **Asistente Inteligente Manuelita**\n"
    "Versi√≥n 1.0 - Memoria conversacional con enrutamiento RAG/Structured"
)

# Ejecutar p√°gina
if page == "üí¨ Chat":
    page_chat()
elif page == "‚öôÔ∏è Admin":
    page_admin()
elif page == "‚ùì FAQs":
    page_faqs()

# ============================================================================
# FOOTER
# ============================================================================

st.sidebar.divider()
st.sidebar.caption(
    "üîß Powered by LangChain + Streamlit\n"
    "Memory: FIFO (20K tokens) | RAG: Hybrid Search + Reranking"
)
